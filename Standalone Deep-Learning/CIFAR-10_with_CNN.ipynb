{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result를 저장할 폴더 생성\n",
    "# !mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy  # Add Deepcopy for args\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "# valloader = torch.utils.data.DataLoader(valset, batch_size=4, shuffle=False)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # in_channels : 채널 수 (RGB가 3차원 이니까 3)\n",
    "        # out_channels : output tensor의 채널 개수, 그 layer의 filter개수와 같음, 보통 2의 x승으로 사용)\n",
    "        # kernel_size : filter size와 동일, 3X3 filter -> 3\n",
    "        # strid : filter를 몇 칸씩 이동 시킬 것인지, Default -> 1\n",
    "        # padding : zero-padding을 얼마나 줄 것이지, Default -> 0. tensor의 H과 W의 사이즈 변동이 없으려면 2로 나눈 몫으로 설정)\n",
    "        # bias : bias 항을 추가할 것인지, Default -> True       \n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3,\n",
    "                                     out_channels = 64,\n",
    "                                     kernel_size = 3,\n",
    "                                     stride = 1,\n",
    "                                     padding = 1)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64,\n",
    "                                     out_channels = 256,\n",
    "                                     kernel_size = 5,\n",
    "                                     stride = 1,\n",
    "                                     padding = 2) # dimension을 유지하기 위해서는 5를 2로 나눈 몫인 2로함? 유지하기를 원하면 kernel과 stride를 보고 계산해서 함\n",
    "        self.acti = nn.ReLU()\n",
    "        self.maxpool_layer1 = nn.MaxPool2d(kernel_size = 2,\n",
    "                                     stride = 2) # strid 수는 계산해서 하는 건가? 내가 정하는 것!\n",
    "        self.fc_layer = nn.Linear(65536, 10) # (input dimension, output dimension-> 10개의 확률값)\n",
    "        \n",
    "    def forward(self, x): # x.shape -> (2, 3, 32, 32)\n",
    "        x = self.conv_layer1(x)   # x.shape -> (2, 64, 32, 32) 32가 유지되는 이유 : dimension을 유지하기 위해 padding을 1로 줬기 때문\n",
    "        x = self.acti(x)   # x.shape -> (2, 64, 32, 32)\n",
    "        x = self.conv_layer2(x) # x.shape -> (2, 256, 32, 32) 32가 유지되는 이유 : dimension을 유지하기 위해 padding을 2로 줬기 때문\n",
    "        x = self.acti(x)  # x.shape -> (2, 256, 32, 32)\n",
    "        x = self.maxpool_layer1(x) # x.shape -> (2, 256, 16, 16)\n",
    "        x = x.view(x.size(0), -1) # fc에 넣기 위해 조정. batch_size는 그대로 유지하고, 나머지 값은 하나로 펴줌 # x.shape -> (2, 256*16*16)\n",
    "        x = self.fc_layer(x) # x.shape -> (2, 10)\n",
    "        return x                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_check():\n",
    "    net = CNN()\n",
    "    x = torch.randn(2, 3, 32, 32) # torch.randn() : 명시한 차원에 따라서 랜덤 값을 만들어줌\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# maxpool 안 했을 때\n",
    "dimension_check() # 여기서 32, 32가 나오는 이유는 kernel size가 5라서..? 아님 그냥 바로 위 셀에서 32라고 지정해줘서?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# maxpool 넣었을 때\n",
    "dimension_check() # depth 는 256, 사이즈만 16이 됐네?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 65536])\n"
     ]
    }
   ],
   "source": [
    "# view 해줬을 때. 이게 linear layer의 input dimension이 됨\n",
    "dimension_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                              batch_size=args.train_batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train() # train할 때만 적용되는 작업(dropout 등)을 수행 \n",
    "    \n",
    "   \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    for i , data in enumerate(trainloader):\n",
    "        optimizer.zero_grad() # tensor에는 tensor 자체의 값과 그 tensor의 gradient가 저장되어 있음. 새로 계산된 gradient가 변질되지 않도록 0으로 만들어 줌\n",
    "        \n",
    "        # get the inputs\n",
    "        inputs, labels = data # inputs.shape -> (train_batch_size,3,32,32)\n",
    "        # inputs = inputs.cuda()\n",
    "        # labels = labels.cuda() # labels.shape -> (train_batch_size,1) \n",
    "        outputs = net(inputs) # outputs.shape -> (train_batch_size, 10)\n",
    "        \n",
    "        loss = criterion(outputs, labels) # batch 마다 각각 계산한 뒤 평균낸 값\n",
    "        loss.backward() # Gradient 계산 : tensor에 저장되어 있는 gradient 값을 바꿔줌\n",
    "        optimizer.step() # 저장되어있는 gradient 값을 기준으로 tensor 자체의 값을 바꿔줌\n",
    "        \n",
    "        train_loss += loss.item() # tensor 자체가 flow를 갖고 있어서 무거움. 그 안에 있는 숫자만 빼와서 누적함 \n",
    "        _, predicted = torch.max(outputs.data, 1) # _ : max 값 / 안 쓰는 변수를 저장, predicted : index 값(몇 번째 index에 max가 있는지), predicted.shape -> (train_batch_size, 1(max의 index))\n",
    "        total += labels.size(0) # labels.shape -> (train_batxh_size, 1), 결국 전체 몇 개의 데이터가 있는지 저장\n",
    "        correct += (predicted == labels).sum().item() # True의 개수. tensor이기 때문에 item 함\n",
    "   \n",
    "    train_loss = train_loss / len(trainloader) # 전체 데이터들의 평균 loss 값\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc # net은 왜 return 하지? 안 해도됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "predicted = torch.tensor([1,2,3])\n",
    "labels = torch.tensor([1,6,9])\n",
    "(predicted == labels).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = [1,2,3]\n",
    "labels = [1,6,9]\n",
    "predicted == labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
    "                                            batch_size=args.test_batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): # gradient를 아예 계산하지 않도록. flow를 계산하지 않도록. -> 속도랑 메모리 향상\n",
    "        for i, data in enumerate(valloader):\n",
    "            images, labels = data\n",
    "            # images = images.cuda()\n",
    "            # labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(val_loss.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad:\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # images = images.cuda()\n",
    "            # labels = labels.cuda()\n",
    "            \n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted, labels).sum().item()\n",
    "            \n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "    \n",
    "    net = CNN()\n",
    "    # net.cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2) # weight_decay가 l2 값을 얼마나 적용할지었나?\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice') # try except 구문도 많이 사용하나? \n",
    "    \n",
    "    train_losses_lst = []\n",
    "    val_losses_lst = []\n",
    "    train_accs_lst = []\n",
    "    val_accs_lst = []\n",
    "    \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time() # time.perf_counter()를 권장 -> cpu 시간이 얼마나 걸렸는지 체크. 이 코드가 얼마나 걸렸는지 알려줌. time.time()보다 좀 더 정확 : 현재 시간\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses_lst.append(train_loss)  # 얘네가 들어가는 과정...?\n",
    "        val_losses_lst.append(val_loss)\n",
    "        train_accs_lst.append(train_acc)\n",
    "        val_accs_lst.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took')\n",
    "        \n",
    "    test_acc = test(net, partition, args)\n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses_lst\n",
    "    result['val_losses'] = val_losses_lst\n",
    "    result['train_accs'] = train_accs_lst\n",
    "    result['val_accs'] = val_accs_lst\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result # vars(args)는 뭐지? args 타입을 dict로 바꿔주는 것. args를 print 하거나, 저장하기 위함\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====Random Seed Initialization===== #\n",
    "seed = 123\n",
    "np.random.seed(seed) # np의 random 함수에 대한 seed\n",
    "torch.manual_seed(seed) # torch의 random 함수에 대한 seed\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = 'exp1_n_layer_hid_dim'\n",
    "\n",
    "# =====Model Capcity===== #\n",
    "args.out_dim = 10\n",
    "\n",
    "args.act = 'relu'\n",
    "\n",
    "# ====Regularization==== #\n",
    "args.l2 = 0.00001\n",
    "# args.use_xavier = True # 파라미터 초기값의 방법 중 하나. 많이 쓰진 않음. 보통 torch가 default값을 알아서 적용하도록 함\n",
    "\n",
    "# =====Optimizer & Training===== #\n",
    "args.optim = 'RMSprop'\n",
    "args.lr = 0.0015\n",
    "args.epoch = 10\n",
    "\n",
    "args.train_batch_size = 256\n",
    "args.test_batch_size = 1024\n",
    "\n",
    "# =====Experiment Variable===== #\n",
    "name_var1 = 'lr'\n",
    "name_var2 = 'l2'\n",
    "list_var1 = [0.0001]\n",
    "list_var2 = [0.00001]\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1) # setattr이 뭐지\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "        \n",
    "        setting, result = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49de8edb4a174572adfce8885130fd6486c82b1df7252289157b68155bf6ea64"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('AI_exam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
